from torch.utils.data import Dataset

import os
import pandas as pd
from sklearn.model_selection import train_test_split
import torch


class MLPCSVTrafficDataset(Dataset):
    def __init__(self, data_list):
        """
        Initializes the dataset with a list of pre-processed data samples.
        Each sample in `data_list` should be a dictionary containing:
        - 'features': a torch.Tensor of numerical feature vectors
        - 'labels': a torch.Tensor (long) for local (base) class index
        - 'global_labels': a torch.Tensor (long) for global class index
        - 'raw_text': (Optional) A string representation of features for LLM input.
        """
        self.data_list = data_list

    def __len__(self):
        return len(self.data_list)

    def __getitem__(self, idx):
        """
        Retrieves a sample from the dataset at the given index.
        """
        return self.data_list[idx]
    

def loadData(args, full_traffic_labels_list, all_class_labels_global_map, ood_labels_to_exclude=[], prefused_data_list=None):
    """
    Args:
        args: An argparse.Namespace object containing configuration parameters
              like `dataset_name`, `SAMPLES_PER_CLASS`, `DEVICE`, etc.
        full_traffic_labels_list: A list of all possible traffic class labels (strings).
        all_class_labels_global_map: A dictionary mapping string labels to their global integer IDs.
        ood_labels_to_exclude: A list of string labels to be considered OOD for the current run.
        prefused_data_list (list): A list of dictionaries containing pre-fused 'fused_features'
                                   and 'global_labels' as generated by the `fuse_features_and_cache` function.
                                   If None, it tries to load from cache or raise an error if not found.

    Returns:
        tuple: (train_dataset, val_dataset, test_dataset, updated_args, base_class_global_indices_sorted)
    """
    dataset_name = args.dataset_name
    num_samples_per_class = args.SAMPLES_PER_CLASS


    base_traffic_labels_str = [label for label in full_traffic_labels_list if label not in ood_labels_to_exclude]
    new_traffic_labels_str = [label for label in full_traffic_labels_list if label in ood_labels_to_exclude]
    dataset_root='/home/icdm/code/trafficCOOP/datasetsM'

    DEFAULT_COMPONENTS = ["textual_emb", "numerical_feats",
                          "explanation_emb", "prior_logits"]
    selected_components = getattr(args, "SELECTED_COMPONENTS", DEFAULT_COMPONENTS)
    selected_components = [str(c) for c in selected_components]

    component_order = ["textual_emb", "numerical_feats",
                       "explanation_emb", "prior_logits"]
    comp_id = 0
    for i, comp in enumerate(component_order):
        if comp in selected_components:
            comp_id |= (1 << i)

    split_cache_file = os.path.join(
        dataset_root, dataset_name, "raw",
        f"splits_{comp_id}_SPC{num_samples_per_class}.pt"
    )
    if os.path.exists(split_cache_file):
        print(f"Loading cached train/val/test splits from: {split_cache_file}")
        cached_splits = torch.load(split_cache_file)

        args.INPUT_DIM = cached_splits["train"][0]["features"].shape[0]
        print(f"[Splitâ€‘Cache] Restored INPUT_DIM={args.INPUT_DIM}")

        args.NUM_BASE_CLASSES = len(base_traffic_labels_str)
        args.NUM_ALL_CLASSES  = len(all_class_labels_global_map)

        train_dataset = MLPCSVTrafficDataset(cached_splits["train"])
        val_dataset   = MLPCSVTrafficDataset(cached_splits["val"])
        test_dataset  = MLPCSVTrafficDataset(cached_splits["test"])

        return train_dataset, val_dataset, test_dataset, args, sorted(
            [all_class_labels_global_map[l] for l in base_traffic_labels_str]
        )

    all_processed_data = prefused_data_list


    first_sample = all_processed_data[0]
    dim_sum = 0
    for comp in selected_components:
        dim_sum += first_sample[comp].shape[0]
    args.INPUT_DIM = dim_sum
    print(f"MLP Input Dimension (Dynamic Fusion): {args.INPUT_DIM}")

    args.NUM_BASE_CLASSES = len(base_traffic_labels_str)
    args.NUM_ALL_CLASSES = len(all_class_labels_global_map)

    base_class_global_indices_sorted = sorted([all_class_labels_global_map[l] for l in base_traffic_labels_str])
    
    temp_df = pd.DataFrame([
        {
            'textual_emb':      sample.get('textual_emb'),
            'numerical_feats':  sample.get('numerical_feats'),
            'explanation_emb':  sample.get('explanation_emb'),
            'prior_logits':     sample.get('prior_logits'),
            'label':            sample['global_labels'].item(),
            'raw_text':         sample.get('raw_text', "")
        }
        for sample in all_processed_data
    ])
    label_counts = temp_df['label'].value_counts().sort_index()
    print(label_counts)

    min_samples_for_stratify_df = temp_df['label'].value_counts().min() >= 2 and len(temp_df['label'].unique()) > 1
    df_train_val_pool, df_test_full = train_test_split(
        temp_df, test_size=0.3, stratify=temp_df['label'] if min_samples_for_stratify_df else None, random_state=42
    )

    df_base_pool = df_train_val_pool[df_train_val_pool['label'].isin(base_class_global_indices_sorted)].copy()
    
    new_class_global_indices = [all_class_labels_global_map[l] for l in new_traffic_labels_str]
    df_new_pool = df_train_val_pool[df_train_val_pool['label'].isin(new_class_global_indices)].copy()

    df_train_decoop = pd.DataFrame(columns=df_base_pool.columns)

    for global_label_idx in base_class_global_indices_sorted:
        class_df = df_base_pool[df_base_pool['label'] == global_label_idx]
        
        if len(class_df) < num_samples_per_class:
            print(f"Warning: Not enough samples ({len(class_df)}) for global class {global_label_idx} to get {num_samples_per_class} for train. Taking all available.")
            train_samples = class_df.sample(frac=1, random_state=42)
        else:
            train_samples = class_df.sample(n=num_samples_per_class, random_state=42)
        
        df_train_decoop = pd.concat([df_train_decoop, train_samples], ignore_index=True)

    train_indices_used = df_train_decoop.index
    df_remaining_base_for_val = df_base_pool.drop(train_indices_used, errors='ignore')
    df_val = pd.concat([df_remaining_base_for_val, df_new_pool], ignore_index=True).sample(frac=1, random_state=42)

    df_test = df_test_full

    print(f"Train: {len(df_train_decoop)} samples")
    print(f"Val: {len(df_val)} samples")
    print(f"Test: {len(df_test)} samples")
    
    def df_to_mlp_dataset_format(dataframe, is_training_data_flag, base_class_global_indices_local=None):
        processed_list = []
        global_to_local_base_map = None
        if is_training_data_flag:
            sorted_base_class_global_indices = sorted(list(set(base_class_global_indices_local)))
            global_to_local_base_map = {global_idx: local_idx for local_idx, global_idx in enumerate(sorted_base_class_global_indices)}

        for idx, row in dataframe.iterrows():
            global_label_numerical = row['label']

            target_label_numerical = global_label_numerical
            if is_training_data_flag:
                target_label_numerical = global_to_local_base_map.get(global_label_numerical, -1)

            parts = []
            for comp in selected_components:
                part = row[comp]
                tensor_part = part if isinstance(part, torch.Tensor) \
                              else torch.tensor(part, dtype=torch.float32)
                parts.append(tensor_part)
            features_tensor = torch.cat(parts, dim=0)

            processed_list.append({
                "features": features_tensor,
                "labels": torch.tensor(target_label_numerical, dtype=torch.long),
                "raw_text": row['raw_text'],
                "global_labels": torch.tensor(global_label_numerical, dtype=torch.long),
                "prior_logits": row['prior_logits']
            })
        return processed_list

    train_numerical_list = df_to_mlp_dataset_format(df_train_decoop, is_training_data_flag=True, base_class_global_indices_local=base_class_global_indices_sorted)
    val_numerical_list = df_to_mlp_dataset_format(df_val, is_training_data_flag=False)
    test_numerical_list = df_to_mlp_dataset_format(df_test, is_training_data_flag=False)

    train_dataset = MLPCSVTrafficDataset(train_numerical_list)
    val_dataset = MLPCSVTrafficDataset(val_numerical_list)
    test_dataset = MLPCSVTrafficDataset(test_numerical_list)


    split_cache_file = os.path.join(
        dataset_root, dataset_name, "raw",
        f"splits_{comp_id}_SPC{num_samples_per_class}.pt"
    )

    torch.save(
        {
            "train": train_numerical_list,
            "val":   val_numerical_list,
            "test":  test_numerical_list
        },
        split_cache_file
    )

    return train_dataset, val_dataset, test_dataset, args, base_class_global_indices_sorted

